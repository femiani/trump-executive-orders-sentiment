import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import contractions

def clean_text(text):
    """
    Extensively cleans a text string:
      - Expands contractions
      - Removes URLs
      - Removes non-alphabetic characters
      - Converts text to lowercase
      - Removes extra whitespace
      - Removes English stopwords
      - Lemmatizes words
    """
    if not isinstance(text, str):
        return ""
    
    # Fix encoding issues
    text = text.encode('ascii', 'ignore').decode('ascii')

    # Expand contractions
    text = contractions.fix(text)
    
    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'/r/\w+', '', text)  # Remove subreddit links
    
    # Remove non-alphabet characters (keeping spaces)
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Remove Reddit-specific markup
    text = re.sub(r'\&gt\;|\&lt\;|\&amp\;', '', text)  # HTML entities
    text = re.sub(r'\[.*?\]\(.*?\)', '', text)  # Markdown links
    text = re.sub(r'\*{1,3}', '', text)  # Bold/italic markers
    
    # Convert to lowercase and remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip().lower()
    
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = text.split()
    words = [word for word in words if word not in stop_words]
    
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]
    
    return " ".join(words)

# Load scraped data from CSV files
df_election = pd.read_csv("election_posts.csv")
df_executive = pd.read_csv("executive_posts.csv")

# Define a list of text columns to clean if they exist
text_columns = ['post_title', 'comment_body']

# Function to clean specified columns in a DataFrame and add new cleaned columns
def clean_dataframe(df):
    for col in text_columns:
        if col in df.columns:
            df[f'clean_{col}'] = df[col].apply(clean_text)
    # Drop only the columns that were cleaned
    df.drop(columns=[col for col in text_columns if col in df.columns], inplace=True)
    return df

# Clean both DataFrames
df_election = clean_dataframe(df_election)
df_executive = clean_dataframe(df_executive)

# Save the cleaned data to new CSV files
df_election.to_csv("election_posts_clean.csv", index=False)
df_executive.to_csv("executive_posts_clean.csv", index=False)

print("Cleaned data saved to 'election_posts_clean.csv' and 'executive_posts_clean.csv'")
